N-BEATS:

N-BEATS (Non-linear forecasting with basis functions) is a neural network architecture for time series forecasting.

The main idea behind N-BEATS is to decompose a complex time series into a sum of simpler functions, called basis functions, which can be learned by a neural network.

N-BEATS consists of two main components: the first is a stack of fully connected layers called the "backcast" model, and the second is a stack of fully connected layers called the "forecast" model. The backcast model is responsible for learning the history of the time series up to the current time step, while the forecast model is responsible for predicting future values.

The backcast and forecast models are trained separately, but they share the same set of weights. This allows the model to learn complex interactions between the past and the future of the time series.

The backcast model is composed of a stack of fully connected layers, called "blocks", each one with a specific number of neurons. Each block takes as input the concatenation of the past values of the time series and the output of the previous block. The output of the last block is then used as input for the forecast model.

The forecast model is also composed of a stack of fully connected layers, called "heads". Each head takes as input the output of the backcast model and predicts a future value of the time series. The final output of the N-BEATS is the sum of the predictions of all heads.

By this way the model learns the non-linear function that best approximate the target series, it can handle multiple input and output sequences, and is particularly well-suited for long-term forecasting.

HYPERPARAMETER TUNNING:

The specific hyperparameters that you should tune for your N-BEATS model will depend on the characteristics of your dataset and the specific problem you're trying to solve. However, here are a few hyperparameters that are commonly tuned for N-BEATS and similar models:

Number of blocks: The number of blocks in the backcast model of the N-BEATS architecture. This parameter controls the depth of the model.

Number of neurons per block: The number of neurons per block in the backcast model of the N-BEATS architecture. This parameter controls the capacity of the model.

Number of heads: The number of heads in the forecast model of the N-BEATS architecture. This parameter controls the number of parallel outputs of the model.

Number of neurons per head: The number of neurons per head in the forecast model of the N-BEATS architecture. This parameter controls the capacity of each output of the model.

Learning rate: The learning rate of the optimizer used during the training process. This parameter controls the step size of the gradient descent algorithm.

Dropout rate: The dropout rate applied to the input of each block and head in the N-BEATS architecture. This parameter helps to prevent overfitting.

Number of training epochs: The number of times the model is trained on the entire dataset.

It's also important to keep in mind that the performance of the model will depend on many factors, such as the quality of the data, the preprocessing techniques used, and the hyperparameters of the model. Therefore, it would be a good idea to use techniques such as cross validation and grid search to find the best set of hyperparameters for your specific dataset.

Input & Output Layers:

- The sizes of the input and output layers should be adequate to assign a node to each feature.

- The input chunk length should not be smaller than the order of seasonality.

- For efficient memory usage, set them to a power of 2

Batch Size:

- Number of observations the model will process before it updates its matrix weights.

- For efficient memory usage, set them to a power of 2.

- Very large batch sizes may mislead the gradient descent in a single direction (sub-optimal minimum)

- Smaller batch sizes will cause the gradient descent to bounce around in different directions

- This reduces accuracy, but can prevent overfitting.

- Usually you should choose an initial batch size of 32.

Epochs:

- Tells the model how many training cycles it is supposed to run.

- During each epoch, the model will process the entire training set, making one forward and one

backward pass.

The product of these hyperparameters defines the tensor size of the model.

- Large parameter values can make it bump against the memory limit of your system and will lead to

exponentially longer processing times.

- Small parameter values may turn out to be inadequate to mirror complex patterns in the source data.

def optimize_nbeats(params):

# create the N-BEATS model with the given set of hyperparameters

nbeats = NBEATSModel(

backcast_length=params['backcast_length'],

forecast_length=params['forecast_length'],

stack_types=[params['stack_type']],

nb_blocks_per_stack=params['nb_blocks_per_stack'],

share_weights_in_stack=params['share_weights_in_stack'],

hidden_layer_units=params['hidden_layer_units']

)

# fit the model on the training data

nbeats.fit(
    x_train,
    y_train,
    batch_size=params['batch_size'],
    epochs=params['epochs'],
    verbose=0
)

# evaluate the model on the validation data

y_pred = nbeats.predict(x_val)

mse = mean_squared_error(y_val, y_pred)

# return the negative of the evaluation metric

return -mse

search_space = {
    'backcast_length': hp.quniform('backcast_length', 10, 100, 10),
    'forecast_length': hp.quniform('forecast_length', 10, 100, 10),
    'stack_type': hp.choice('stack_type', ['linear', 'dense']),
    'nb_blocks_per_stack': hp.quniform('nb_blocks_per_stack', 1, 3, 1),
    'share_weights_in_stack': hp.choice('share_weights_in_stack', [True, False]),
    'hidden_layer_units': hp.quniform('hidden_layer_units', 10, 100, 10),
    'batch_size': hp.quniform('batch_size', 32, 256, 32),
    'epochs': hp.quniform('epochs', 10, 100, 10)
}

best = fmin(fn=optimize_nbeats, space=search_space, algo=None)