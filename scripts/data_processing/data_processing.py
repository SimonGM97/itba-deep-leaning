from PyTradeX.config.params import Params
from PyTradeX.utils.general.logging_helper import get_logger
from PyTradeX.utils.others.scripts_helper import (
    force_intervals,
    retrieve_params,
    log_params,
    reset_intervals
)
from PyTradeX.data_processing.data_processor import data_processing_job
import argparse
import json
from pprint import pformat


def lambda_handler(
    event: dict, 
    context: dict = None
) -> dict:
    """
    :param `event`: (dict) Data sent during lambda function invocation.
    :param `context`: (dict) Generated by the platform and contains information about the underlying infrastructure
        and execution environment, such as allowed runtime and memory.
    """
    # Log event
    LOGGER.info('event:\n%s\n', pformat(event))

    if "AWS_LAMBDA_EVENT_BODY" in event.keys():
        # Access the payload from the event parameter
        payload_str = event.get("AWS_LAMBDA_EVENT_BODY")

        # Parse the JSON payload
        payload: dict = json.loads(payload_str)

        # Extract parameters
        workflow = payload.get("workflow", "default")
        forced_intervals = payload.get("forced_intervals", None)
    else:
        # Extract parameters
        workflow = event.get("workflow", "default")
        forced_intervals = event.get("forced_intervals", None)

    # Force intervals
    force_intervals(
        logger=LOGGER,
        forced_intervals=forced_intervals
    )
    
    # Retrieve parameters
    params = retrieve_params(workflow=workflow)
    
    # Show params
    log_params(
        logger=LOGGER, 
        log_keys=[
            'intervals', 'full_coin_list', 'update_client',
            'update_correlations', 'update_lc_ids', 'overwrite',
            'collective_data_update_params', 'data_extractor_update_params',
            'data_cleaner_update_params', 'data_shifter_update_params',
            'data_refiner_update_params', 'selected_features_update_params',
            'data_transformer_update_params'
        ],
        **params
    )
    
    # Show context
    LOGGER.info('context:\n%s\n', pformat(context))
    
    # try:
    data_processing_job(**params)

    # Re-set intervals
    reset_intervals(
        logger=LOGGER,
        forced_intervals=forced_intervals
    )

    return {
        'statusCode': 200,
        'body': json.dumps('Data Processing job ran successfully!')
    }


# Get logger
LOGGER = get_logger(
    name=__name__,
    level=Params.log_params.get('level'),
    txt_fmt=Params.log_params.get('txt_fmt'),
    json_fmt=Params.log_params.get('json_fmt'),
    filter_lvls=Params.log_params.get('filter_lvls'),
    log_file=Params.log_params.get('log_file'),
    backup_count=Params.log_params.get('backup_count')
)


# conda deactivate
# source .itba_dl/bin/activate
# .itba_dl/bin/python scripts/data_processing/data_processing.py --workflow trading_round
#   --workflow: default | model_building | trading_round | model_updating | repair
#   --forced_intervals: 30min | 60min
if __name__ == "__main__":
    # Define parser
    parser = argparse.ArgumentParser(description='Data processing script.')

    # Add workflow & forced_intervals arguments
    parser.add_argument(
        '--workflow',
        type=str,
        default='default',
        choices=['default', 'model_building', 'trading_round', 'model_updating', 'repair']
    )
    parser.add_argument(
        '--forced_intervals',
        type=str,
        default=None,
        choices=['30min', '60min', None]
    )

    # Extract arguments from parser
    args = parser.parse_args()
    workflow: str = args.workflow
    forced_intervals: str = args.forced_intervals

    # Define event
    event: dict = {
        'workflow': workflow,
        'forced_intervals': forced_intervals
    }
    
    # Run lambda function
    lambda_handler(event=event, context=None)
